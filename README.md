# Data Engineering ZoomCamp 2025

## Overview
Data engineering involves the development and maintenance of architectures such as databases and large-scale processing systems. A data engineer is responsible for designing, building, and managing the infrastructure that allows for the collection, storage, and analysis of data. This role is crucial for ensuring that data is accessible, reliable, and ready for downstream consumption by data scientists and analysts.

Data engineering is a foundational aspect of the data lifecycle, enabling data science by providing clean, organized, and well-structured data. Data scientists rely on this infrastructure to perform analyses, build models, and derive insights that drive business decisions.

This project aims to provide a comprehensive understanding of data engineering concepts and tools through practical modules. Each module focuses on different technologies and practices essential for modern data engineering:

- **Containerization and Infrastructure as Code**: Using Docker and Terraform to create reproducible environments and manage infrastructure.
- **Workflow Orchestration**: Understanding how to automate and manage data workflows using tools like Kestra.
- **Data Ingestion**: Exploring techniques for reading data from APIs, normalizing it, and ensuring scalability.
- **Data Warehousing**: Gaining insights into using BigQuery for large-scale data storage and analysis.
- **Analytics Engineering**: Using dbt for transforming data and Metabase for visualization.
- **Batch Processing**: Learning about Apache Spark for processing large datasets efficiently.
- **Streaming**: Real-time data processing with Kafka and schema management with Avro.

These technologies are critical for building robust data pipelines and ensuring that data is processed efficiently and accurately, ultimately supporting data-driven decision-making within organizations.

## Modules

### Module 1: Containerization and Infrastructure as Code
- Introduction to GCP
- Docker and Docker Compose
- Running PostgreSQL with Docker
- Infrastructure setup with Terraform
- Homework

### Module 2: Workflow Orchestration
- Data Lakes and Workflow Orchestration
- Workflow orchestration with Kestra
- Homework

### Workshop 1: Data Ingestion
- API reading and pipeline scalability
- Data normalization and incremental loading
- Homework

### Module 3: Data Warehousing
- Introduction to BigQuery
- Partitioning, clustering, and best practices
- Machine learning in BigQuery

### Module 4: Analytics Engineering
- dbt (data build tool) with PostgreSQL & BigQuery
- Testing, documentation, and deployment
- Data visualization with Metabase

### Module 5: Batch Processing
- Introduction to Apache Spark
- DataFrames and SQL
- Internals of GroupBy and Joins

### Module 6: Streaming
- Introduction to Kafka
- Kafka Streams and KSQL
- Schema management with Avro

### Final Project
- Apply all concepts learned in a real-world scenario
- Peer review and feedback process